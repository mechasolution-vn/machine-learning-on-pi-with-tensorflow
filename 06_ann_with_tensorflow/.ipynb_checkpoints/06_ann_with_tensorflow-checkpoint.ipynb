{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dự án 01: Xây dựng Raspberry PI thành máy tính cho Data Scientist (PIDS)\n",
    "## Bài 06. Thử nghiệm Artificial Neural Network (ANN) với TensorFlow\n",
    "\n",
    "##### Người soạn: Dương Trần Hà Phương\n",
    "##### Website: [Mechasolution Việt Nam](https://mechasolution.vn)\n",
    "##### Email: mechasolutionvietnam@gmail.com\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tensorflow](https://tensorflow.org/) (TF) là một thư viện mã nguồn mở của Google đang rất được cộng đồng học thuật quan tâm, đặc biệt HOT trong lĩnh vực Deep Learning. TF được thiết kế để có khả năng tính toán dễ dàng trên các đồ thị tính toán hay các mạng nơron nhân tạo - Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifical  Neural Network (ANN) là gì ?\n",
    "\n",
    "Mạng nơron nhân tạo là một trong những công cụ chính được sử dụng trong học máy. Từ \"nơron\" trong tên gọi được lấy cảm hứng từ bộ não của con người,  được thiết kế để bắt chước theo cách mà con người chúng ta học. Mạng nơron bao gồm nhiều lớp (layers):\n",
    "* lớp đầu vào (input layer)\n",
    "* lớp ẩn (hidden layer): Một lớp ẩn bao gồm các phép biến đổi lớp đầu vào thành thứ gì đó mà lớp đầu ra có thể sử dụng. \n",
    "* lớp đầu ra (output layer)\n",
    "\n",
    "ANN là những công cụ tuyệt vời cho việc tìm kiếm kết quả trong các bài toán quá phức tạp hoặc quá nhiều đối với con người.\n",
    "\n",
    "Ví dụ về ANN:\n",
    "\n",
    "![Sample ANN](B05474_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giới thiệu Single Layer Perceptron (SLP)\n",
    "SLP là mô hình Neural Network (NN) đầu tiền được đề xuất vào năm 1958 bởi Frank Rosenblatt. Hàm số xác định class của Perceptron là $label(x) = sgn(w^Tx)$ có thể được mô tả như hình vẽ (được gọi là network) dưới đây:\n",
    "\n",
    "![Single Layer Perceptron](pla_nn.png)\n",
    "\n",
    "Đầu vào của network $x$ được minh họa bằng các node màu xanh lục với node $x_0$ luôn luôn bằng 1. Tập hợp các node màu xanh lục được gọi là Input layer. Trong ví dụ này, tôi giả sử số chiều của dữ liệu $d=4$. Số node trong input layer luôn luôn là $d+1$ với một node là 1 được thêm vào. Node $x_0=1$ này đôi khi được ẩn đi.\n",
    "\n",
    "Các trọng số (weights) $w_0, w_1,…,w_d$ được gán vào các mũi tên đi tới node $z = \\sum_{i=0}^{d} w_ix_i = w^Tx$\n",
    "\n",
    "Node $y = sgn(z)$ là output của network. Ký hiệu hình chữ Z ngược màu xanh trong node $y$ thể hiện đồ thị của hàm số $sgn$.\n",
    "\n",
    "Hàm số $y = sgn(z)$ còn được gọi là activation function. Đây chính là dạng đơn giản nhất của Neural Network.\n",
    "\n",
    "Các Neural Networks sau này có thể có nhiều node ở output tạo thành một output layer, hoặc có thể có thêm các layer trung gian giữa input layer và output layer. Các layer trung gian đó được gọi là hidden layer. Khi biểu diễn các Networks lớn, người ta thường giản lược hình bên trái thành hình bên phải. Trong đó node $x_0 = 1$ thường được ẩn đi. Node $z$ cũng được ẩn đi và viết gộp vào trong node $y$. Perceptron thường được vẽ dưới dạng đơn giản như hình bên dưới.\n",
    "\n",
    "![Biểu diễn của Linear Regression dưới dạng Neural Network.](lr_nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Các kiến thức liên quan\n",
    "Trước khi bước qua phần cái đặt ANN với TensorFlow, ta sẽ tìm hiểu một số kiến thức liên quan có sử dụng trong ANN như: Activation function, Softmax function, Cross Entropy error\n",
    "\n",
    "#### 1. Activation function\n",
    "Một mỗi node của NN đóng vai trò như một nơron trong mạng nơron sinh học. Mỗi node hoạt động khi và chỉ khi tổng giá trị nó nhận được từ lớp trước đó vượt quá ngưỡng kích hoạt. Hàm số thực hiện việc đó gọi là **activation function**.\n",
    "\n",
    "Một số **activation function** thông dụng như: ReLU (Rectified Linear Unit), Sigmoid, Tanh, Softmax.\n",
    "\n",
    "#### 2. Softmax function\n",
    "Trong toán học, hàm softmax, hoặc hàm trung bình mũ là sự khái quát hóa của hàm lôgit biến không gian K-chiều vector với giá trị thực bất kỳ đến không gian K-chiều vector mang giá trị trong phạm vi (0, 1] bao gồm cả giá trị 1.\n",
    "\n",
    "Trong lý thuyết xác suất, giá trị xuất ra của hàm softmax có thể được sử dụng để đại diện cho một loại phân phối – đó là phân phối xác xuất trên K khả năng khác nhau có thể xảy ra. Trong thực tế, nó là gradien logarit chuẩn hóa thuộc nhóm phân phối xác suất.\n",
    "\n",
    "![Mô hình Softmax Regression dưới dạng Neural network.](softmax_nn.png)\n",
    "\n",
    "\n",
    "#### 3. Cross Entropy error\n",
    "Cross entropy giữa hai phân phối $p$ và $q$ được định nghĩa là khoảng cách giữa hai phân phối xác suất với công thức: $H(p,q) = E_p[-logq]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cài đặt ANN với TensorFlow\n",
    "\n",
    "Bây giờ chúng ta sẽ bắt đầu làm một ví dụ tạo một ANN đơn giản với 3 lớp với TensorFlow. Trong ví dụ này, chúng ta sẽ sử dụng dataset MNIST, đây là dataset được TensorFlow cung cấp. Dataset MNIST là một tập hợp của 28x28 pixel ảnh grayscale của rất nhiều chữ số viết tay. Dataset này bao gồm 55,000 dòng cho training, 10,000 dòng cho testing và 5,000 dòng cho validation.\n",
    "\n",
    "![OUr neural network](TensorFlow-data-flow-graph.gif)\n",
    "\n",
    "Chúng ta có thể load dataset bằng cách chạy 2 lệnh sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tham số `one_hot=True` sẽ biểu diễn nhãn bằng một vector với tất cả phần từ bằng 0 ngoại trừ phần tử có chỉ số (id) bằng với nhãn. Ví dụ: với nhãn \"4\" thì ta có vector \"one hot\" như sau: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0].\n",
    "\n",
    "#### 1. Cài đặt tham số cho mô hình\n",
    "\n",
    "Tiếp theo, chúng ta sẽ cài đặt những placeholder variables cho việc huấn luyện dữ liệu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python optimisation variables\n",
    "learning_rate = 0.5\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "# declare the training data placeholders\n",
    "# input x - for 28 x 28 pixels = 784\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# now declare the output data placeholder - 10 digits\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lưu ý: x - input layer với 784 nodes thể hiện cho 28 x 28 (=784) pixels, và y - output layer với 10 nodes thể hiện 10 giá trị có thể của 10 chữ số (0, 1, 2, . . ., 9). Một lần nữa kích thước của x là (? x 784) với ? là một tham số chưa biết giá trị đầu vào, giá trị này sẽ thay đổi tuỳ thuộc vào placeholder variable.\n",
    "\n",
    "Bây giờ chúng ta cần cài đặt biến trọng số và bias cho 3 lớp của ANN. Luôn có L-1 số lượng tensor thể hiện trọng số/bias, với L là số lớp. Vì vậy trong trường hợp này, ta cần cài đặt 2 tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now declare the weights connecting the input to the hidden layer\n",
    "W1 = tf.Variable(tf.random_normal([784, 300], stddev=0.03), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([300]), name='b1')\n",
    "\n",
    "# and the weights connecting the hidden layer to the output layer\n",
    "W2 = tf.Variable(tf.random_normal([300, 10], stddev=0.03), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='b2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, hãy cũng nhau tìm hiểu về đoạn code ở trên một tí nhé ! Đầu tiền, ta khai báo biến W1 và b2, trọng số và bias cho các liên kết giữa lớp input và lớp ẩn. Mạng nơron này sẽ có 300 nodes tại lớp ẩn, vì vậy kích thước của tensor W1 là [784, 300]. Ngoài ra, chúng ta còn khai báo giá trị của trọng số - W là một phân phối chuẩn ngẫu nhiên với mean = 0 và độ lệch chuẩn = 0.03. Tương tự, chúng ta tạo các biến W2 và b2 để nối lớp ẩn với lớp đầu ra của mạng nơron của chúng ta.\n",
    "\n",
    "Tiếp theo, chúng ta cần phải cài đặt node đầu vào và activation function cho các node của lớp ẩn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the output of the hidden layer\n",
    "hidden_out = tf.add(tf.matmul(x, W1), b1)\n",
    "hidden_out = tf.nn.relu(hidden_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở dòng đầu tiên, chúng ta thực hiện phép nhân ma trận giữa ma trận trọng số (W1) với vector input (x) và ta cộng kết quả với bias (b1). Phép nhân ma trận ở đây được thực hiện bởi toán tử `tf.matmul`. Tiếp theo, áp dụng activation function ReLU (Rectified Linear Unit) vào lớp ẩn với kết quả nhân ma trận ở trên. Lưu ý, TensorFlow có hỗ trợ sẵn cho chúng ta activation function ReLU - `tf.nn.relu`\n",
    "\n",
    "Hai dòng trên là hiện thực theo 2 công thức sau đây:\n",
    "\n",
    "$z^{(l+1)} = W^{(l)}x + b^{(l)}$\n",
    "\n",
    "$h^{(l+1)} = f(z^{(l+1)})$\n",
    "\n",
    "Bây giờ, ta sẽ cài đặt lớp output và y_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now calculate the hidden layer output - in this case, let's use a softmax activated\n",
    "# output layer\n",
    "y_ = tf.nn.softmax(tf.add(tf.matmul(hidden_out, W2), b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một lần nữa, chúng ta sẽ sử dụng activation function là Softmax cho lớp output - `tf.nn.softmax`. Chúng ta cũng phải sử dụng một hàm để tính lỗi hoặc độ chính xác cho bài toán tối ưu. Ở đây chúng ta sẽ sử dụng Cross Entropy, công thức như sau:\n",
    "\n",
    "$J = -\\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^n y_j^{(i)}log(y_j\\_^{(i)}) + (1 – y_j^{(i)})log(1 – y_j\\_^{(i)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-483b4178483e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_clipped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.9999999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n\u001b[0;32m      3\u001b[0m                          + (1 - y) * tf.log(1 - y_clipped), axis=1))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n",
    "                         + (1 - y) * tf.log(1 - y_clipped), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giải thích code:\n",
    "- Dòng đầu tiên, ta chuyển output y_ về trong đoạn 1e-10 to 0.999999. Điều này sẽ giúp ta tránh trường hợp log(0) trong quá trình huấn luyện. Nếu gặp log(0) thì sẽ trả về NaN và ngưng quá trình huấn luyện ngay lặp tức. \n",
    "- Dòng thứ 2, tính Cross entropy\n",
    "\n",
    "Hàm `tf.reduce_sum` tính tổng của một chiều nhất định của tensor mà bạn cung cấp. Trong trường hợp này, tensor được thực hiện phép tính cross-entropy trên một node và tập huấn luyện: $y_j^{(i)}log(y_j\\_^{(i)}) + (1 – y_j^{(i)})log(1 – y_j\\_^{(i)})$. Hãy nhớ rằng y và y_clipped trong phép tính trên là (m x 10) tensors - do đó chúng ta cần thực hiện tổng đầu tiên trên chiều thứ hai. Điều này được xác định bằng cách sử dụng axis = 1, trong đó “1” có nghĩa là chúng ta sử dụng chiều thứ hai vì Python sử dụng chỉ số bắt đầu từ 0.\n",
    "\n",
    "Sau thao tác này, chúng ta có một tensor (m x 1). Để lấy giá trị trung bình của tensor này và hoàn thành phép tính cross entropy (tức là thực hiện phần này $\\frac{1}{m} \\sum_{i=1}^m$), chúng ta sử dụng hàm `tf.reduce_mean` của TensorFlow. Hàm này đơn giản lấy giá trị trung bình của bất kỳ tensor nào mà bạn cung cấp. Vì vậy, bây giờ chúng ta có một cost function có thể sử dụng trong quá trình huấn luyện.\n",
    "\n",
    "Hãy thiết lập trình tối ưu hóa trong TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an optimiser\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở đây chúng ta chỉ dùng Gradient Descent optimiser được cung cấp bởi TensorFlow và khởi tạo tham số learning rate. Sau đó, ta minimize giá trị của Cross Entropy mà chúng ta đã tính ở trên.\n",
    "\n",
    "Cuối cùng, trước khi chúng ta chuyển sang phần chính của chương trình, hãy khởi tạo các biến và các toán tử để ước lượng độ chính xác của quá trình dự đoán:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally setup the initialisation operator\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# define an accuracy assessment operation\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cài đặt cho quá trình huấn luyện (training)\n",
    "\n",
    "Vậy là chúng ta đã có tất cả mọi thứ cần thiết để cài đặt quá trình huấn luyện neural network. Bắt đầu quá trình huấn luận bằng code dưới đây:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 0.665\n",
      "Epoch: 2 cost = 0.247\n",
      "Epoch: 3 cost = 0.181\n",
      "Epoch: 4 cost = 0.149\n",
      "Epoch: 5 cost = 0.125\n",
      "Epoch: 6 cost = 0.103\n",
      "Epoch: 7 cost = 0.088\n",
      "Epoch: 8 cost = 0.076\n",
      "Epoch: 9 cost = 0.065\n",
      "Epoch: 10 cost = 0.060\n",
      "Average accuracy: 0.9729\n"
     ]
    }
   ],
   "source": [
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "   # initialise the variables\n",
    "   sess.run(init_op)\n",
    "   total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "\n",
    "   for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "            _, c = sess.run([optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y})\n",
    "            avg_cost += c / total_batch\n",
    "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "        \n",
    "   print(\"Average accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giải thích code:\n",
    "- Dòng 2: Bắt đầu khởi tạo Session trong TensorFlow để chạy các phép toán đã chuẩn bị phía trên\n",
    "- Dòng 4: Chạy các toán tử đã khởi tạo.\n",
    "- Dòng 5: tính số batch (lô) sẽ chạy trong mỗi epoch của tập huấn luyện.\n",
    "- Dòng 7 - cuối: lặp qua từng epoch của tập huấn luyện và khởi tạo biến avg_cost để theo dõi giá trị trung bình của Cross entropy cho mỗi epoch. Dòng tiếp theo là nơi chúng ta trích xuất một lô mẫu ngẫu nhiên, batch_x và batch_y, từ tập dữ liệu đào tạo MNIST. TensorFlow cung cấp một phương thức vô cùng tiện dụng trong bộ dữ liệu MNIST đó là `next_batch`, giúp chúng ta dễ dàng trích xuất các lô dữ liệu cho việc huấn luyện."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuối cùng, chúng ta đã thu được độ chính xác xấp xỉ 98% trên tập test. Chúng ta có thể làm một số thứ để nâng độ chính xác như regularisation, nhưng ở đây chúng ta chỉ dừng ở bước áp dụng TensorFlow để xây dựng một ANN nên việc tăng độ chính xác các bạn hãy tự tìm hiểu thêm nhé !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tham khảo\n",
    "[1] [Perceptron Learning Algorithm](https://machinelearningcoban.com/2017/01/21/perceptron/#-mo-hinh-neural-network-dau-tien)\n",
    "\n",
    "[2] [Softmax Regression](https://machinelearningcoban.com/2017/01/21/perceptron/#-mo-hinh-neural-network-dau-tien)\n",
    "\n",
    "[3] [Python TensorFlow Tutorial – Build a Neural Network](http://adventuresinmachinelearning.com/python-tensorflow-tutorial/)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "Nếu có thắc mắc hoặc góp ý, các bạn hãy comment bên dưới để bài viết có thể được hoàn thiện hơn. \n",
    "Xin cảm ơn,\n",
    "\n",
    "Hà Phương - Mechasolution Việt Nam."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
