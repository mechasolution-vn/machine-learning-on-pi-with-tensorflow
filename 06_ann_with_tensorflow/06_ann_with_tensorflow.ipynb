{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dự án 01: Xây dựng Raspberry PI thành máy tính cho Data Scientist (PIDS)\n",
    "## Bài 06. Thử nghiệm Artificial Neural Network (ANN) với TensorFlow\n",
    "\n",
    "##### Người soạn: Dương Trần Hà Phương\n",
    "##### Website: [Mechasolution Việt Nam](https://mechasolution.vn)\n",
    "##### Email: mechasolutionvietnam@gmail.com\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tensorflow](https://tensorflow.org/) (TF) là một thư viện mã nguồn mở của Google đang rất được cộng đồng học thuật quan tâm, đặc biệt HOT trong lĩnh vực Deep Learning. TF được thiết kế để có khả năng tính toán dễ dàng trên các đồ thị tính toán hay các mạng nơron nhân tạo - Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifical  Neural Network (ANN) là gì ?\n",
    "\n",
    "Mạng nơron nhân tạo là một trong những công cụ chính được sử dụng trong học máy. Từ \"nơron\" trong tên gọi được lấy cảm hứng từ bộ não của con người,  được thiết kế để bắt chước theo cách mà con người chúng ta học. Mạng nơron bao gồm nhiều lớp (layers):\n",
    "* lớp đầu vào (input layer)\n",
    "* lớp ẩn (hidden layer): Một lớp ẩn bao gồm các phép biến đổi lớp đầu vào thành thứ gì đó mà lớp đầu ra có thể sử dụng. \n",
    "* lớp đầu ra (output layer)\n",
    "\n",
    "ANN là những công cụ tuyệt vời cho việc tìm kiếm kết quả trong các bài toán quá phức tạp hoặc quá nhiều đối với con người.\n",
    "\n",
    "Ví dụ về ANN:\n",
    "\n",
    "![Sample ANN](B05474_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giới thiệu Single Layer Perceptron (SLP)\n",
    "SLP là mô hình Neural Network (NN) đầu tiền được đề xuất vào năm 1958 bởi Frank Rosenblatt. Hàm số xác định class của Perceptron là $label(x) = sgn(w^Tx)$ có thể được mô tả như hình vẽ (được gọi là network) dưới đây:\n",
    "\n",
    "![Single Layer Perceptron](pla_nn.png)\n",
    "\n",
    "Đầu vào của network $x$ được minh họa bằng các node màu xanh lục với node $x_0$ luôn luôn bằng 1. Tập hợp các node màu xanh lục được gọi là Input layer. Trong ví dụ này, tôi giả sử số chiều của dữ liệu $d=4$. Số node trong input layer luôn luôn là $d+1$ với một node là 1 được thêm vào. Node $x_0=1$ này đôi khi được ẩn đi.\n",
    "\n",
    "Các trọng số (weights) $w_0, w_1,…,w_d$ được gán vào các mũi tên đi tới node $z = \\sum_{i=0}^{d} w_ix_i = w^Tx$\n",
    "\n",
    "Node $y = sgn(z)$ là output của network. Ký hiệu hình chữ Z ngược màu xanh trong node $y$ thể hiện đồ thị của hàm số $sgn$.\n",
    "\n",
    "Hàm số $y = sgn(z)$ còn được gọi là activation function. Đây chính là dạng đơn giản nhất của Neural Network.\n",
    "\n",
    "Các Neural Networks sau này có thể có nhiều node ở output tạo thành một output layer, hoặc có thể có thêm các layer trung gian giữa input layer và output layer. Các layer trung gian đó được gọi là hidden layer. Khi biểu diễn các Networks lớn, người ta thường giản lược hình bên trái thành hình bên phải. Trong đó node $x_0 = 1$ thường được ẩn đi. Node $z$ cũng được ẩn đi và viết gộp vào trong node $y$. Perceptron thường được vẽ dưới dạng đơn giản như hình bên dưới.\n",
    "\n",
    "![Biểu diễn của Linear Regression dưới dạng Neural Network.](lr_nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Các kiến thức liên quan\n",
    "Trước khi bước qua phần cái đặt ANN với TensorFlow, ta sẽ tìm hiểu một số kiến thức liên quan có sử dụng trong ANN như: Activation function, Softmax function, Cross Entropy error\n",
    "\n",
    "#### 1. Activation function\n",
    "Một mỗi node của NN đóng vai trò như một nơron trong mạng nơron sinh học. Mỗi node hoạt động khi và chỉ khi tổng giá trị nó nhận được từ lớp trước đó vượt quá ngưỡng kích hoạt. Hàm số thực hiện việc đó gọi là **activation function**.\n",
    "\n",
    "Một số **activation function** thông dụng như: ReLU (Rectified Linear Unit), Sigmoid, Tanh, Softmax.\n",
    "\n",
    "#### 2. Softmax function\n",
    "Trong toán học, hàm softmax, hoặc hàm trung bình mũ là sự khái quát hóa của hàm lôgit biến không gian K-chiều vector với giá trị thực bất kỳ đến không gian K-chiều vector mang giá trị trong phạm vi (0, 1] bao gồm cả giá trị 1.\n",
    "\n",
    "Trong lý thuyết xác suất, giá trị xuất ra của hàm softmax có thể được sử dụng để đại diện cho một loại phân phối – đó là phân phối xác xuất trên K khả năng khác nhau có thể xảy ra. Trong thực tế, nó là gradien logarit chuẩn hóa thuộc nhóm phân phối xác suất.\n",
    "\n",
    "![Mô hình Softmax Regression dưới dạng Neural network.](softmax_nn.png)\n",
    "\n",
    "\n",
    "#### 3. Cross Entropy error\n",
    "Cross entropy giữa hai phân phối $p$ và $q$ được định nghĩa là khoảng cách giữa hai phân phối xác suất với công thức: $H(p,q) = E_p[-logq]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cài đặt ANN với TensorFlow\n",
    "\n",
    "Bây giờ chúng ta sẽ bắt đầu làm một ví dụ tạo một ANN đơn giản với 3 lớp với TensorFlow. Trong ví dụ này, chúng ta sẽ sử dụng dataset MNIST, đây là dataset được TensorFlow cung cấp. Dataset MNIST là một tập hợp của 28x28 pixel ảnh grayscale của rất nhiều chữ số viết tay. Dataset này bao gồm 55,000 dòng cho training, 10,000 dòng cho testing và 5,000 dòng cho validation.\n",
    "\n",
    "Chúng ta có thể load dataset bằng cách chạy 2 lệnh sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tham số `one_hot=True` sẽ biểu diễn nhãn bằng một vector với tất cả phần từ bằng 0 ngoại trừ phần tử có chỉ số (id) bằng với nhãn. Ví dụ: với nhãn \"4\" thì ta có vector \"one hot\" như sau: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0].\n",
    "\n",
    "#### 1. Cài đặt tham số cho mô hình\n",
    "\n",
    "Tiếp theo, chúng ta sẽ cài đặt những placeholder variables cho việc huấn luyện dữ liệu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python optimisation variables\n",
    "learning_rate = 0.5\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "# declare the training data placeholders\n",
    "# input x - for 28 x 28 pixels = 784\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# now declare the output data placeholder - 10 digits\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lưu ý: x - input layer với 784 nodes thể hiện cho 28 x 28 (=784) pixels, và y - output layer với 10 nodes thể hiện 10 giá trị có thể của 10 chữ số (0, 1, 2, . . ., 9). Một lần nữa kích thước của x là (? x 784) với ? là một tham số chưa biết giá trị đầu vào, giá trị này sẽ thay đổi tuỳ thuộc vào placeholder variable.\n",
    "\n",
    "Bây giờ chúng ta cần cài đặt biến trọng số và bias cho 3 lớp của ANN. Luôn có L-1 số lượng tensor thể hiện trọng số/bias, với L là số lớp. Vì vậy trong trường hợp này, ta cần cài đặt 2 tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now declare the weights connecting the input to the hidden layer\n",
    "W1 = tf.Variable(tf.random_normal([784, 300], stddev=0.03), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([300]), name='b1')\n",
    "\n",
    "# and the weights connecting the hidden layer to the output layer\n",
    "W2 = tf.Variable(tf.random_normal([300, 10], stddev=0.03), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='b2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so let’s unpack the above code a little.  First, we declare some variables for W1 and b1, the weights and bias for the connections between the input and hidden layer.  This neural network will have 300 nodes in the hidden layer, so the size of the weight tensor W1 is [784, 300].  We initialise the values of the weights using a random normal distribution with a mean of zero and a standard deviation of 0.03.  TensorFlow has a replicated version of the numpy random normal function, which allows you to create a matrix of a given size populated with random samples drawn from a given distribution.  Likewise, we create W2 and b2 variables to connect the hidden layer to the output layer of the neural network.\n",
    "\n",
    "Next, we have to setup node inputs and activation functions of the hidden layer nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the output of the hidden layer\n",
    "hidden_out = tf.add(tf.matmul(x, W1), b1)\n",
    "hidden_out = tf.nn.relu(hidden_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first line, we execute the standard matrix multiplication of the weights (W1) by the input vector x and we add the bias b1.  The matrix multiplication is executed using the tf.matmul operation.  Next, we finalise the hidden_out operation by applying a rectified linear unit activation function to the matrix multiplication plus bias.  Note that TensorFlow has a rectified linear unit activation already setup for us, tf.nn.relu.\n",
    "\n",
    "This is to execute the following equations, as detailed in the neural networks tutorial:\n",
    "\n",
    "$z^{(l+1)} = W^{(l)}x + b^{(l)}$\n",
    "\n",
    "$h^{(l+1)} = f(z^{(l+1)})$\n",
    "\n",
    "Now, let’s setup the output layer, y_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now calculate the hidden layer output - in this case, let's use a softmax activated\n",
    "# output layer\n",
    "y_ = tf.nn.softmax(tf.add(tf.matmul(hidden_out, W2), b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we perform the weight multiplication with the output from the hidden layer (hidden_out) and add the bias, b2.  In this case, we are going to use a softmax activation for the output layer – we can use the included TensorFlow softmax function tf.nn.softmax.\n",
    "\n",
    "We also have to include a cost or loss function for the optimisation / backpropagation to work on. Here we’ll use the cross entropy cost function, represented by:\n",
    "\n",
    "$J = -\\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^n y_j^{(i)}log(y_j\\_^{(i)}) + (1 – y_j^{(i)})log(1 – y_j\\_^{(i)})$\n",
    "\n",
    "Where $y_{j}^{(i)}$ is the ith training label for output node j, $y_j\\_^{(i)}$ is the ith predicted label for output node j, m is the number of training / batch samples and n is the number .  There are two operations occurring in the above equation.  The first is the summation of the logarithmic products and additions across all the output nodes.  The second is taking a mean of this summation across all the training samples.  We can implement this cross entropy cost function in TensorFlow with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y * tf.log(y_clipped)\n",
    "                         + (1 - y) * tf.log(1 - y_clipped), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some explanation is required.  The first line is an operation converting the output y_ to a clipped version, limited between 1e-10 to 0.999999.  This is to make sure that we never get a case were we have a log(0) operation occurring during training – this would return NaN and break the training process.  The second line is the cross entropy calculation.\n",
    "\n",
    "To perform this calculation, first we use TensorFlow’s tf.reduce_sum function – this function basically takes the sum of a given axis of the tensor you supply.  In this case, the tensor that is supplied is the element-wise cross-entropy calculation for a single node and training sample i.e.: $y_j^{(i)}log(y_j\\_^{(i)}) + (1 – y_j^{(i)})log(1 – y_j\\_^{(i)})$.\n",
    "Remember that y and y_clipped in the above calculation are (m x 10) tensors – therefore we need to perform the first sum over the second axis.  This is specified using the axis=1 argument, where “1” actually refers to the second axis when we have a zero-based indices system like Python.\n",
    "\n",
    "After this operation, we have an (m x 1) tensor.  To take the mean of this tensor and complete our cross entropy cost calculation (i.e. execute this part $\\frac{1}{m} \\sum_{i=1}^m$), we use TensorFlow’s tf.reduce_mean function.  This function simply takes the mean of whatever tensor you provide it.  So now we have a cost function that we can use in the training process.\n",
    "\n",
    "Let’s setup the optimiser in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an optimiser\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are just using the gradient descent optimiser provided by TensorFlow.  We initialize it with a learning rate, then specify what we want it to do – i.e. minimise the cross entropy cost operation we created.  This function will then perform the gradient descent (for more details on gradient descent see here and here) and the backpropagation for you.  How easy is that?  TensorFlow has a library of popular neural network training optimisers, see here.\n",
    "\n",
    "Finally, before we move on to the main show, were we actually run the operations, let’s setup the variable initialisation operation and an operation to measure the accuracy of our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally setup the initialisation operator\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# define an accuracy assessment operation\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct prediction operation correct_prediction makes use of the TensorFlow tf.equal function which returns True or False depending on whether to arguments supplied to it are equal.  The tf.argmax function is the same as the numpy argmax function, which returns the index of the maximum value in a vector / tensor.  Therefore, the correct_prediction operation returns a tensor of size (m x 1) of True and False values designating whether the neural network has correctly predicted the digit.  We then want to calculate the mean accuracy from this tensor – first we have to cast the type of the correct_prediction operation from a Boolean to a TensorFlow float in order to perform the reduce_mean operation.  Once we’ve done that, we now have an accuracy operation ready to assess the performance of our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cài đặt cho quá trình huấn luyện (training)\n",
    "\n",
    "We now have everything we need to setup the training process of our neural network.  I’m going to show the full code below, then talk through it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 0.665\n",
      "Epoch: 2 cost = 0.247\n",
      "Epoch: 3 cost = 0.181\n",
      "Epoch: 4 cost = 0.149\n",
      "Epoch: 5 cost = 0.125\n",
      "Epoch: 6 cost = 0.103\n",
      "Epoch: 7 cost = 0.088\n",
      "Epoch: 8 cost = 0.076\n",
      "Epoch: 9 cost = 0.065\n",
      "Epoch: 10 cost = 0.060\n",
      "Average accuracy: 0.9729\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "_epoch = np.array([0])\n",
    "_acc = np.array([0])\n",
    "\n",
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "   # initialise the variables\n",
    "   sess.run(init_op)\n",
    "   total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "\n",
    "   for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)\n",
    "            _, c = sess.run([optimiser, cross_entropy], \n",
    "                         feed_dict={x: batch_x, y: batch_y})\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        _epoch = np.append(_epoch, [epoch + 1])\n",
    "        _acc = np.append(_acc, [avg_cost])\n",
    "\n",
    "        print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(avg_cost))\n",
    "        \n",
    "   print(\"Average accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stepping through the lines above, the first couple relate to setting up the with statement and running the initialisation operation.  The third line relates to our mini-batch training scheme that we are going to run for this neural network.  If you want to know about mini-batch gradient descent, check out this post.  In the third line, we are calculating the number of batches to run through in each training epoch.  After that, we loop through each training epoch and initialise an avg_cost variable to keep track of the average cross entropy cost for each epoch.  The next line is where we extract a randomised batch of samples, batch_x and batch_y, from the MNIST training dataset.  The TensorFlow provided MNIST dataset has a handy utility function, next_batch, that makes it easy to extract batches of data for training.\n",
    "\n",
    "The following line is where we run two operations.  Notice that sess.run is capable of taking a list of operations to run as its first argument.  In this case, supplying [optimiser, cross_entropy] as the list means that both these operations will be performed.  As such, we get two outputs, which we have assigned to the variables _ and c.  We don’t really care too much about the output from the optimiser operation but we want to know the output from the cross_entropy operation – which we have assigned to the variable c.  Note, we run the optimiser (and cross_entropy) operation on the batch samples.  In the following line, we use c to calculate the average cost for the epoch.\n",
    "\n",
    "Finally, we print out our progress in the average cost, and after the training is complete, we run the accuracy operation to print out the accuracy of our trained network on the test set.  Running this program produces the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go – approximately 98% accuracy on the test set, not bad.  We could do a number of things to improve the model, such as regularisation (see this tips and tricks post), but here we are just interested in exploring TensorFlow.  You can also use TensorBoard visualisation to look at things like the increase in accuracy over the epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] [0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(_epoch, _acc)\n",
    "plt.plot(_epoch, _acc)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy %')\n",
    "plt.title('The plot below shown the increase in accuracy over 10 epochs')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tham khảo\n",
    "[1] [Perceptron Learning Algorithm](https://machinelearningcoban.com/2017/01/21/perceptron/#-mo-hinh-neural-network-dau-tien)\n",
    "\n",
    "[2] [Softmax Regression](https://machinelearningcoban.com/2017/01/21/perceptron/#-mo-hinh-neural-network-dau-tien)\n",
    "\n",
    "[3] [Python TensorFlow Tutorial – Build a Neural Network](http://adventuresinmachinelearning.com/python-tensorflow-tutorial/)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "Nếu có thắc mắc hoặc góp ý, các bạn hãy comment bên dưới để bài viết có thể được hoàn thiện hơn. \n",
    "Xin cảm ơn,\n",
    "\n",
    "Hà Phương - Mechasolution Việt Nam."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
